nohup: ignoring input
Namespace(dataset='cora', dropout=0.5, early_stopping=10, epochs=400, hidden=16, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
  (0, 633)	1
  (0, 1862)	1
  (0, 2582)	1
  (1, 2)	1
  (1, 652)	1
  (1, 654)	1
  (2, 1)	1
  (2, 332)	1
  (2, 1454)	1
  (2, 1666)	1
  (2, 1986)	1
  (3, 2544)	1
  (4, 1016)	1
  (4, 1256)	1
  (4, 1761)	1
  (4, 2175)	1
  (4, 2176)	1
  (5, 1629)	1
  (5, 1659)	1
  (5, 2546)	1
  (6, 373)	1
  (6, 1042)	1
  (6, 1416)	1
  (6, 1602)	1
  (7, 208)	1
  :	:
  (2694, 431)	1
  (2694, 2695)	1
  (2695, 431)	1
  (2695, 2694)	1
  (2696, 2615)	1
  (2697, 986)	1
  (2698, 1400)	1
  (2698, 1573)	1
  (2699, 2630)	1
  (2700, 1151)	1
  (2701, 44)	1
  (2701, 2624)	1
  (2702, 186)	1
  (2702, 1536)	1
  (2703, 1298)	1
  (2704, 641)	1
  (2705, 287)	1
  (2706, 165)	1
  (2706, 169)	1
  (2706, 1473)	1
  (2706, 2707)	1
  (2707, 165)	1
  (2707, 598)	1
  (2707, 1473)	1
  (2707, 2706)	1
adj: (2708, 2708)
features: (2708, 1433)
y: (2708, 7) (2708, 7) (2708, 7)
mask: (2708,) (2708,) (2708,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],
                       [1274, 1247, 1194,  ...,  329,  186,   19]]),
       values=tensor([0.1111, 0.1111, 0.1111,  ..., 0.0769, 0.0769, 0.0769]),
       device='cuda:0', size=(2708, 1433), nnz=49216, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,  633, 1862,  ..., 1473, 2706, 2707],
                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),
       values=tensor([0.2458, 0.2529, 0.2190,  ..., 0.2025, 0.2001, 0.1899]),
       device='cuda:0', size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)
input dim: 1433
output dim: 7
num_features_nonzero: 49216
0 13.34626293182373 0.12857143580913544
10 11.57707691192627 0.19285713136196136
20 10.109631538391113 0.3214285671710968
30 8.83889389038086 0.264285683631897
40 7.7302093505859375 0.38571426272392273
50 6.841838359832764 0.44999998807907104
60 6.087249755859375 0.5214285850524902
70 5.383449077606201 0.5928571224212646
80 4.823485374450684 0.5571428537368774
90 4.2780680656433105 0.5857142806053162
100 3.861171245574951 0.6714285612106323
110 3.4444360733032227 0.7285714149475098
120 3.0756664276123047 0.8071428537368774
130 2.7553741931915283 0.8714285492897034
140 2.541001081466675 0.8500000238418579
150 2.2742152214050293 0.8857142329216003
160 2.072070598602295 0.8714285492897034
170 1.990586757659912 0.8714285492897034
180 1.848132610321045 0.9071428775787354
190 1.6502658128738403 0.9428570866584778
200 1.6835194826126099 0.8785713911056519
210 1.5194170475006104 0.9357141852378845
220 1.4541070461273193 0.928571343421936
230 1.4560492038726807 0.8857142329216003
240 1.3368840217590332 0.9357141852378845
250 1.3277348279953003 0.9071428775787354
260 1.2838337421417236 0.9357142448425293
270 1.2389366626739502 0.8928571343421936
280 1.168534517288208 0.9357141852378845
290 1.1056334972381592 0.9357141852378845
300 1.1446330547332764 0.9357142448425293
310 1.122010588645935 0.928571343421936
320 1.131077766418457 0.9071428775787354
330 1.0674912929534912 0.949999988079071
340 1.095564842224121 0.9214285612106323
350 1.0759875774383545 0.9428570866584778
360 1.0650944709777832 0.9571428298950195
370 1.0500025749206543 0.9357142448425293
380 1.0064576864242554 0.9714285731315613
390 1.0667059421539307 0.928571343421936
test: 0.8220000267028809
