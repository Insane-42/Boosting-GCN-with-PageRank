Namespace(dataset='cora', dropout=0.5, early_stopping=10, epochs=400, hidden=16, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
  (0, 633)	1
  (0, 1862)	1
  (0, 2582)	1
  (1, 2)	1
  (1, 652)	1
  (1, 654)	1
  (2, 1)	1
  (2, 332)	1
  (2, 1454)	1
  (2, 1666)	1
  (2, 1986)	1
  (3, 2544)	1
  (4, 1016)	1
  (4, 1256)	1
  (4, 1761)	1
  (4, 2175)	1
  (4, 2176)	1
  (5, 1629)	1
  (5, 1659)	1
  (5, 2546)	1
  (6, 373)	1
  (6, 1042)	1
  (6, 1416)	1
  (6, 1602)	1
  (7, 208)	1
  :	:
  (2694, 431)	1
  (2694, 2695)	1
  (2695, 431)	1
  (2695, 2694)	1
  (2696, 2615)	1
  (2697, 986)	1
  (2698, 1400)	1
  (2698, 1573)	1
  (2699, 2630)	1
  (2700, 1151)	1
  (2701, 44)	1
  (2701, 2624)	1
  (2702, 186)	1
  (2702, 1536)	1
  (2703, 1298)	1
  (2704, 641)	1
  (2705, 287)	1
  (2706, 165)	1
  (2706, 169)	1
  (2706, 1473)	1
  (2706, 2707)	1
  (2707, 165)	1
  (2707, 598)	1
  (2707, 1473)	1
  (2707, 2706)	1
adj: (2708, 2708)
features: (2708, 1433)
y: (2708, 7) (2708, 7) (2708, 7)
mask: (2708,) (2708,) (2708,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],
                       [1274, 1247, 1194,  ...,  329,  186,   19]]),
       values=tensor([0.1111, 0.1111, 0.1111,  ..., 0.0769, 0.0769, 0.0769]),
       device='cuda:0', size=(2708, 1433), nnz=49216, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,  633, 1862,  ..., 1473, 2706, 2707],
                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),
       values=tensor([0.3963, 0.2023, 0.1781,  ..., 0.1694, 0.1666, 0.3224]),
       device='cuda:0', size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)
input dim: 1433
output dim: 7
num_features_nonzero: 49216
0 13.354036331176758 0.11428571492433548
10 11.58736515045166 0.1785714328289032
20 10.117006301879883 0.2928571403026581
30 8.84882640838623 0.2571428716182709
40 7.739747524261475 0.37857139110565186
50 6.85869836807251 0.4285714626312256
60 6.0953049659729 0.5
70 5.402209758758545 0.5714285373687744
80 4.836904048919678 0.5428571105003357
90 4.303311347961426 0.5714285373687744
100 3.861372709274292 0.6571428179740906
110 3.4703500270843506 0.6928570866584778
120 3.08957576751709 0.8214285969734192
130 2.7623634338378906 0.8642857670783997
140 2.567613124847412 0.8214285969734192
150 2.2861685752868652 0.8785713911056519
160 2.0667686462402344 0.8857142329216003
170 2.009369134902954 0.8428570628166199
180 1.8694055080413818 0.8928571343421936
190 1.6475436687469482 0.9357142448425293
200 1.6534266471862793 0.9142857193946838
210 1.5245580673217773 0.9214285612106323
220 1.469179391860962 0.9214285612106323
230 1.437366008758545 0.8785713911056519
240 1.3301465511322021 0.928571343421936
250 1.332442283630371 0.9071428179740906
260 1.2557764053344727 0.9428570866584778
270 1.2266393899917603 0.8999999761581421
280 1.1707518100738525 0.9499999284744263
290 1.0927852392196655 0.928571343421936
300 1.134562373161316 0.949999988079071
310 1.114173412322998 0.9428570866584778
320 1.1174240112304688 0.9214285612106323
330 1.0791290998458862 0.9214285612106323
340 1.086341381072998 0.9214285612106323
350 1.0530238151550293 0.9785714149475098
360 1.0366755723953247 0.9714285731315613
370 1.0349199771881104 0.949999988079071
380 0.9874639511108398 0.985714316368103
390 1.0395828485488892 0.928571343421936
test: 0.8109999895095825
