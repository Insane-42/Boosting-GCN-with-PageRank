Namespace(dataset='cora', dropout=0.5, early_stopping=10, epochs=400, hidden=16, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
  (0, 633)	1
  (0, 1862)	1
  (0, 2582)	1
  (1, 2)	1
  (1, 652)	1
  (1, 654)	1
  (2, 1)	1
  (2, 332)	1
  (2, 1454)	1
  (2, 1666)	1
  (2, 1986)	1
  (3, 2544)	1
  (4, 1016)	1
  (4, 1256)	1
  (4, 1761)	1
  (4, 2175)	1
  (4, 2176)	1
  (5, 1629)	1
  (5, 1659)	1
  (5, 2546)	1
  (6, 373)	1
  (6, 1042)	1
  (6, 1416)	1
  (6, 1602)	1
  (7, 208)	1
  :	:
  (2694, 431)	1
  (2694, 2695)	1
  (2695, 431)	1
  (2695, 2694)	1
  (2696, 2615)	1
  (2697, 986)	1
  (2698, 1400)	1
  (2698, 1573)	1
  (2699, 2630)	1
  (2700, 1151)	1
  (2701, 44)	1
  (2701, 2624)	1
  (2702, 186)	1
  (2702, 1536)	1
  (2703, 1298)	1
  (2704, 641)	1
  (2705, 287)	1
  (2706, 165)	1
  (2706, 169)	1
  (2706, 1473)	1
  (2706, 2707)	1
  (2707, 165)	1
  (2707, 598)	1
  (2707, 1473)	1
  (2707, 2706)	1
adj: (2708, 2708)
features: (2708, 1433)
y: (2708, 7) (2708, 7) (2708, 7)
mask: (2708,) (2708,) (2708,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],
                       [1274, 1247, 1194,  ...,  329,  186,   19]]),
       values=tensor([0.1111, 0.1111, 0.1111,  ..., 0.0769, 0.0769, 0.0769]),
       device='cuda:0', size=(2708, 1433), nnz=49216, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,  633, 1862,  ..., 1473, 2706, 2707],
                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),
       values=tensor([0.3946, 0.2037, 0.1764,  ..., 0.1702, 0.1668, 0.3191]),
       device='cuda:0', size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)
input dim: 1433
output dim: 7
num_features_nonzero: 49216
0 13.35129451751709 0.11428571492433548
10 11.587894439697266 0.18571428954601288
20 10.116471290588379 0.2928571403026581
30 8.848021507263184 0.27142855525016785
40 7.740250587463379 0.3928571343421936
50 6.860141754150391 0.4285714030265808
60 6.0966081619262695 0.5071428418159485
70 5.402442455291748 0.5714285373687744
80 4.838528633117676 0.535714328289032
90 4.303125381469727 0.5714285373687744
100 3.866886615753174 0.6642857193946838
110 3.476168155670166 0.6928570866584778
120 3.093806266784668 0.8214285969734192
130 2.767348289489746 0.8714285492897034
140 2.570362091064453 0.8214285969734192
150 2.2912099361419678 0.8714285492897034
160 2.071603536605835 0.8928571343421936
170 2.011385440826416 0.8499999046325684
180 1.871152400970459 0.8928571343421936
190 1.6510555744171143 0.9428572058677673
200 1.6594358682632446 0.9071428775787354
210 1.5255281925201416 0.9214285612106323
220 1.4710874557495117 0.928571343421936
230 1.4404125213623047 0.8857142329216003
240 1.334418773651123 0.928571343421936
250 1.3356661796569824 0.8999999761581421
260 1.2632677555084229 0.9428570866584778
270 1.2311179637908936 0.9142857193946838
280 1.176760196685791 0.9499999284744263
290 1.0969862937927246 0.9357142448425293
300 1.141631841659546 0.949999988079071
310 1.1203277111053467 0.9428572058677673
320 1.1220347881317139 0.9357141852378845
330 1.0824036598205566 0.9285714626312256
340 1.0924108028411865 0.928571343421936
350 1.0575004816055298 0.9785714149475098
360 1.0437878370285034 0.9714285731315613
370 1.0409417152404785 0.949999988079071
380 0.9916553497314453 0.9785714149475098
390 1.0460246801376343 0.9357142448425293
test: 0.8109999895095825
