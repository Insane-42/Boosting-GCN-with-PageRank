nohup: ignoring input
Namespace(dataset='cora', dropout=0.5, early_stopping=10, epochs=400, hidden=16, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
  (0, 633)	1
  (0, 1862)	1
  (0, 2582)	1
  (1, 2)	1
  (1, 652)	1
  (1, 654)	1
  (2, 1)	1
  (2, 332)	1
  (2, 1454)	1
  (2, 1666)	1
  (2, 1986)	1
  (3, 2544)	1
  (4, 1016)	1
  (4, 1256)	1
  (4, 1761)	1
  (4, 2175)	1
  (4, 2176)	1
  (5, 1629)	1
  (5, 1659)	1
  (5, 2546)	1
  (6, 373)	1
  (6, 1042)	1
  (6, 1416)	1
  (6, 1602)	1
  (7, 208)	1
  :	:
  (2694, 431)	1
  (2694, 2695)	1
  (2695, 431)	1
  (2695, 2694)	1
  (2696, 2615)	1
  (2697, 986)	1
  (2698, 1400)	1
  (2698, 1573)	1
  (2699, 2630)	1
  (2700, 1151)	1
  (2701, 44)	1
  (2701, 2624)	1
  (2702, 186)	1
  (2702, 1536)	1
  (2703, 1298)	1
  (2704, 641)	1
  (2705, 287)	1
  (2706, 165)	1
  (2706, 169)	1
  (2706, 1473)	1
  (2706, 2707)	1
  (2707, 165)	1
  (2707, 598)	1
  (2707, 1473)	1
  (2707, 2706)	1
adj: (2708, 2708)
features: (2708, 1433)
y: (2708, 7) (2708, 7) (2708, 7)
mask: (2708,) (2708,) (2708,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],
                       [1274, 1247, 1194,  ...,  329,  186,   19]]),
       values=tensor([0.1111, 0.1111, 0.1111,  ..., 0.0769, 0.0769, 0.0769]),
       device='cuda:0', size=(2708, 1433), nnz=49216, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,  633, 1862,  ..., 1473, 2706, 2707],
                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),
       values=tensor([0.2487, 0.2507, 0.2224,  ..., 0.2008, 0.2000, 0.1968]),
       device='cuda:0', size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)
input dim: 1433
output dim: 7
num_features_nonzero: 49216
0 13.348325729370117 0.12857143580913544
10 11.574662208557129 0.19999998807907104
20 10.110359191894531 0.3285714089870453
30 8.838850975036621 0.27142855525016785
40 7.728148460388184 0.42142853140830994
50 6.840459823608398 0.44285714626312256
60 6.084969520568848 0.535714328289032
70 5.383075714111328 0.5785714387893677
80 4.820566654205322 0.5499999523162842
90 4.278689384460449 0.5785714387893677
100 3.850559949874878 0.6714285612106323
110 3.4330248832702637 0.7357142567634583
120 3.066969871520996 0.8071428537368774
130 2.7451541423797607 0.8642857670783997
140 2.5359115600585938 0.8428570628166199
150 2.263017416000366 0.8857142329216003
160 2.0625481605529785 0.8714285492897034
170 1.9857206344604492 0.8785713911056519
180 1.844954490661621 0.9071428775787354
190 1.6401596069335938 0.9428572058677673
200 1.6696081161499023 0.8999999761581421
210 1.5150179862976074 0.9214285612106323
220 1.4506361484527588 0.9428570866584778
230 1.4467804431915283 0.8857142329216003
240 1.3284220695495605 0.9357141852378845
250 1.3229215145111084 0.9142857193946838
260 1.2730445861816406 0.9357142448425293
270 1.228244662284851 0.8928571343421936
280 1.1555460691452026 0.9499999284744263
290 1.0955398082733154 0.9357141852378845
300 1.1321077346801758 0.949999988079071
310 1.111053466796875 0.9357142448425293
320 1.1236751079559326 0.9214285612106323
330 1.0612704753875732 0.9571428298950195
340 1.0836374759674072 0.9214285612106323
350 1.0669901371002197 0.9428570866584778
360 1.0512995719909668 0.964285671710968
370 1.038251280784607 0.9428570866584778
380 0.9960380792617798 0.9714285731315613
390 1.0518474578857422 0.928571343421936
test: 0.8240000009536743
