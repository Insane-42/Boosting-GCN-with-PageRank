nohup: ignoring input
Namespace(dataset='cora', dropout=0.5, early_stopping=10, epochs=400, hidden=16, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
  (0, 633)	1
  (0, 1862)	1
  (0, 2582)	1
  (1, 2)	1
  (1, 652)	1
  (1, 654)	1
  (2, 1)	1
  (2, 332)	1
  (2, 1454)	1
  (2, 1666)	1
  (2, 1986)	1
  (3, 2544)	1
  (4, 1016)	1
  (4, 1256)	1
  (4, 1761)	1
  (4, 2175)	1
  (4, 2176)	1
  (5, 1629)	1
  (5, 1659)	1
  (5, 2546)	1
  (6, 373)	1
  (6, 1042)	1
  (6, 1416)	1
  (6, 1602)	1
  (7, 208)	1
  :	:
  (2694, 431)	1
  (2694, 2695)	1
  (2695, 431)	1
  (2695, 2694)	1
  (2696, 2615)	1
  (2697, 986)	1
  (2698, 1400)	1
  (2698, 1573)	1
  (2699, 2630)	1
  (2700, 1151)	1
  (2701, 44)	1
  (2701, 2624)	1
  (2702, 186)	1
  (2702, 1536)	1
  (2703, 1298)	1
  (2704, 641)	1
  (2705, 287)	1
  (2706, 165)	1
  (2706, 169)	1
  (2706, 1473)	1
  (2706, 2707)	1
  (2707, 165)	1
  (2707, 598)	1
  (2707, 1473)	1
  (2707, 2706)	1
adj: (2708, 2708)
features: (2708, 1433)
y: (2708, 7) (2708, 7) (2708, 7)
mask: (2708,) (2708,) (2708,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],
                       [1274, 1247, 1194,  ...,  329,  186,   19]]),
       values=tensor([0.1111, 0.1111, 0.1111,  ..., 0.0769, 0.0769, 0.0769]),
       device='cuda:0', size=(2708, 1433), nnz=49216, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,  633, 1862,  ..., 1473, 2706, 2707],
                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),
       values=tensor([0.2471, 0.2518, 0.2203,  ..., 0.2020, 0.2000, 0.1922]),
       device='cuda:0', size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)
input dim: 1433
output dim: 7
num_features_nonzero: 49216
0 13.347362518310547 0.12857143580913544
10 11.576033592224121 0.19285713136196136
20 10.110230445861816 0.3285714089870453
30 8.83867359161377 0.264285683631897
40 7.728891849517822 0.40714284777641296
50 6.840921401977539 0.44285714626312256
60 6.0861310958862305 0.5142857432365417
70 5.382345199584961 0.5785714387893677
80 4.821557521820068 0.5499999523162842
90 4.278376579284668 0.5857142806053162
100 3.8565430641174316 0.6714285612106323
110 3.439377784729004 0.7285714149475098
120 3.07266902923584 0.8071428537368774
130 2.7508363723754883 0.8714285492897034
140 2.539182424545288 0.8428570628166199
150 2.2692525386810303 0.8857142329216003
160 2.0678822994232178 0.8785713911056519
170 1.9884727001190186 0.8714285492897034
180 1.8459360599517822 0.9071428179740906
190 1.6453860998153687 0.9428572058677673
200 1.6766287088394165 0.8785713911056519
210 1.5171502828598022 0.9357141852378845
220 1.4523286819458008 0.9357142448425293
230 1.4518675804138184 0.8857142329216003
240 1.332387924194336 0.9357141852378845
250 1.3252406120300293 0.9142857193946838
260 1.2786906957626343 0.9357142448425293
270 1.2332112789154053 0.8928571343421936
280 1.161811351776123 0.9428570866584778
290 1.100409746170044 0.9357141852378845
300 1.1376845836639404 0.9357142448425293
310 1.1158332824707031 0.928571343421936
320 1.127310037612915 0.9071428775787354
330 1.0640826225280762 0.949999988079071
340 1.0895533561706543 0.9214285612106323
350 1.0709604024887085 0.9428570866584778
360 1.0575848817825317 0.964285671710968
370 1.0443364381790161 0.9357142448425293
380 1.0005073547363281 0.9714285731315613
390 1.0594664812088013 0.928571343421936
test: 0.8240000009536743
