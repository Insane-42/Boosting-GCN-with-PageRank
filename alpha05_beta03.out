nohup: ignoring input
Namespace(dataset='cora', dropout=0.5, early_stopping=10, epochs=400, hidden=16, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
  (0, 633)	1
  (0, 1862)	1
  (0, 2582)	1
  (1, 2)	1
  (1, 652)	1
  (1, 654)	1
  (2, 1)	1
  (2, 332)	1
  (2, 1454)	1
  (2, 1666)	1
  (2, 1986)	1
  (3, 2544)	1
  (4, 1016)	1
  (4, 1256)	1
  (4, 1761)	1
  (4, 2175)	1
  (4, 2176)	1
  (5, 1629)	1
  (5, 1659)	1
  (5, 2546)	1
  (6, 373)	1
  (6, 1042)	1
  (6, 1416)	1
  (6, 1602)	1
  (7, 208)	1
  :	:
  (2694, 431)	1
  (2694, 2695)	1
  (2695, 431)	1
  (2695, 2694)	1
  (2696, 2615)	1
  (2697, 986)	1
  (2698, 1400)	1
  (2698, 1573)	1
  (2699, 2630)	1
  (2700, 1151)	1
  (2701, 44)	1
  (2701, 2624)	1
  (2702, 186)	1
  (2702, 1536)	1
  (2703, 1298)	1
  (2704, 641)	1
  (2705, 287)	1
  (2706, 165)	1
  (2706, 169)	1
  (2706, 1473)	1
  (2706, 2707)	1
  (2707, 165)	1
  (2707, 598)	1
  (2707, 1473)	1
  (2707, 2706)	1
adj: (2708, 2708)
features: (2708, 1433)
y: (2708, 7) (2708, 7) (2708, 7)
mask: (2708,) (2708,) (2708,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],
                       [1274, 1247, 1194,  ...,  329,  186,   19]]),
       values=tensor([0.1111, 0.1111, 0.1111,  ..., 0.0769, 0.0769, 0.0769]),
       device='cuda:0', size=(2708, 1433), nnz=49216, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,  633, 1862,  ..., 1473, 2706, 2707],
                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),
       values=tensor([0.1410, 0.2869, 0.2499,  ..., 0.2234, 0.2222, 0.1063]),
       device='cuda:0', size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)
input dim: 1433
output dim: 7
num_features_nonzero: 49216
0 13.339091300964355 0.11428571492433548
10 11.574869155883789 0.16428571939468384
20 10.107147216796875 0.30000001192092896
30 8.841981887817383 0.25
40 7.737080097198486 0.41428571939468384
50 6.842369079589844 0.4357142746448517
60 6.093766212463379 0.5285714268684387
70 5.389446258544922 0.5714285373687744
80 4.828073501586914 0.535714328289032
90 4.279635906219482 0.6071428060531616
100 3.8744287490844727 0.6571428179740906
110 3.4408669471740723 0.7357142567634583
120 3.081298589706421 0.7999999523162842
130 2.7634098529815674 0.8571428060531616
140 2.5384037494659424 0.8500000238418579
150 2.2831811904907227 0.8928571343421936
160 2.0874109268188477 0.8571428060531616
170 1.989410638809204 0.8785713911056519
180 1.8537592887878418 0.9000000357627869
190 1.6770799160003662 0.9428570866584778
200 1.7090938091278076 0.8500000238418579
210 1.5326485633850098 0.9142857193946838
220 1.4545018672943115 0.8999999761581421
230 1.4842455387115479 0.8785713911056519
240 1.3513548374176025 0.9357141852378845
250 1.328683614730835 0.9071428775787354
260 1.3116428852081299 0.9142857193946838
270 1.25394606590271 0.9071428775787354
280 1.1798385381698608 0.9285714626312256
290 1.1252400875091553 0.928571343421936
300 1.160841464996338 0.928571343421936
310 1.131939172744751 0.928571343421936
320 1.142568826675415 0.9142857193946838
330 1.0715636014938354 0.9571428298950195
340 1.103340744972229 0.928571343421936
350 1.0994689464569092 0.9428570866584778
360 1.0854783058166504 0.9571428298950195
370 1.0645701885223389 0.928571343421936
380 1.0285792350769043 0.964285671710968
390 1.0893118381500244 0.949999988079071
test: 0.8209999799728394
